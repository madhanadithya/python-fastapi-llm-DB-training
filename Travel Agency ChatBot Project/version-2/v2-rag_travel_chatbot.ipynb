{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862c68b5-02e5-4763-a3d8-175185cc616a",
   "metadata": {},
   "source": [
    "**USING LANGGRAPH**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c06e02c-bccd-4e6a-8283-f17beb8705eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5f2faf-ac0a-487c-bc0f-cf7f4f76d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 557, which is longer than the specified 500\n",
      "Created a chunk of size 1493, which is longer than the specified 500\n",
      "Created a chunk of size 685, which is longer than the specified 500\n",
      "Created a chunk of size 665, which is longer than the specified 500\n",
      "Created a chunk of size 767, which is longer than the specified 500\n",
      "Created a chunk of size 653, which is longer than the specified 500\n",
      "Created a chunk of size 713, which is longer than the specified 500\n",
      "Created a chunk of size 605, which is longer than the specified 500\n",
      "Created a chunk of size 890, which is longer than the specified 500\n",
      "Created a chunk of size 1031, which is longer than the specified 500\n",
      "Created a chunk of size 663, which is longer than the specified 500\n",
      "Created a chunk of size 706, which is longer than the specified 500\n",
      "Created a chunk of size 881, which is longer than the specified 500\n",
      "Created a chunk of size 1255, which is longer than the specified 500\n",
      "Created a chunk of size 895, which is longer than the specified 500\n",
      "Created a chunk of size 951, which is longer than the specified 500\n",
      "Created a chunk of size 1277, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 1160, which is longer than the specified 500\n",
      "Created a chunk of size 504, which is longer than the specified 500\n",
      "Created a chunk of size 944, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 926, which is longer than the specified 500\n",
      "C:\\Users\\91637\\AppData\\Local\\Temp\\ipykernel_17104\\2082350511.py:41: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  Chroma(persist_directory=db_name, embedding_function=embedding_function).delete_collection()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 136\n",
      "Document types found: {'flight-information', 'safety-and-emergency-information', 'travel-insurance', 'travel-packages-and-itineraries', 'visa-and-travel-documentation', 'currency-exchange-and-budgeting', 'hotel-information', 'local-transport-and-rentals'}\n",
      "Vectorstore created with 136 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "db_name = \"vector_db\"\n",
    "\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embedding_function).delete_collection()\n",
    "\n",
    "# Creating vectorstore\n",
    "\n",
    "db  = Chroma.from_documents(documents=chunks, embedding=embedding_function, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {db._collection.count()} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2995663-e9cc-4c69-bdaa-c5f4c666bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 136 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = db._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ce5a46-395d-4a03-a01e-dbf92b6e1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac9f78f-d2ef-44f6-8c02-d7912d85fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"You're a lively and witty travel assistant, always ready for a friendly chat! Your goal is to provide concise (1-2 sentence) answers that are informative, humorous, and engaging. Feel free to sprinkle in light jokes or travel tips where appropriate.\n",
    "\n",
    "When responding, follow these guidelines:\n",
    "- Prioritize the latest question while considering relevant context and chat history.\n",
    "- Be polite, positive, and enthusiastic.\n",
    "- Offer practical suggestions with a cheerful tone.\n",
    "- Encourage further conversation with follow-up questions.\n",
    "- If the response involves multiple recommendations, options, or explanations, present them using fun emojis or creative separators like 🚀, 🌟, or ✅ for clarity and engagement.\n",
    "\n",
    "Chathistory: {history}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Respond cheerfully and engagingly:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83d35bf-96bc-4bd6-96ad-dabc415a80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "rag_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6180ec-a624-4bce-a6aa-85d3fe7a8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: float = Field(description=\"Confidence score that the question is about the specified topics. Ranges from 0.0 to 1.0.\")\n",
    "\n",
    "\n",
    "def question_rewriter(state: AgentState):\n",
    "    print(f\"Entering question_rewriter with following state: {state}\")\n",
    "\n",
    "    state[\"documents\"] = []\n",
    "    state[\"on_topic\"] = \"\"\n",
    "    state[\"rephrased_question\"] = \"\"\n",
    "    state[\"proceed_to_generate\"] = False\n",
    "    state[\"rephrase_count\"] = 0\n",
    "\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conversation = state[\"messages\"][:-1]\n",
    "        current_question = state[\"question\"].content\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                # content=\"You are a helpful assistant that rephrases the user's question to be a standalone question optimized for retrieval.\"\n",
    "                content=\"You are a friendly, witty, and engaging travel assistant who loves chatting with users! You rephrase questions in a natural, conversational way while keeping them informative and useful.\"\n",
    "\n",
    "            )\n",
    "        ]\n",
    "        messages.extend(conversation)\n",
    "        messages.append(HumanMessage(content=current_question))\n",
    "        rephrase_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = rephrase_prompt.format()\n",
    "        response = llm.invoke(prompt)\n",
    "        better_question = response.content.strip()\n",
    "        print(f\"question_rewriter: Rephrased question: {better_question}\")\n",
    "        state[\"rephrased_question\"] = better_question\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    print(\"Entering question_classifier\")\n",
    "\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a classifier that determines whether a user's question is about one of the following topics:\n",
    "\n",
    "        1. Flight Information, including booking policies, baggage rules, cancellations, refunds, rescheduling, no-show & missed flight policies, and travel class upgrades.\n",
    "        2. Hotel Information, including booking guidelines, cancellations, deposits, extra charges, peak season pricing, and refund policies.\n",
    "        3. Visa and Travel Documentation, including visa requirements, application processes, rejection reasons, and necessary travel documents.\n",
    "        4. Travel Packages and Itineraries, including package details, popular destinations, sample itineraries, pricing, inclusions, and insurance recommendations.\n",
    "        5. Local Transport and Rentals, including airport transfers, public transport, car rentals, and ride-sharing options.\n",
    "        6. Travel Insurance, including types of coverage, exclusions, and claims processes.\n",
    "        7. Safety and Emergency Information, including emergency contacts, common travel scams, health advisories, and safety tips.\n",
    "        8. Currency Exchange and Budgeting, including exchange options, rate tracking, ATM withdrawals, budgeting, and money safety tips.\n",
    "        \n",
    "        Provide a confidence score from 0.0 to 1.0. A higher score means higher confidence that the question is on one of these topics.\"\"\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"User question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    state[\"on_topic\"] = result.score\n",
    "    print(f\"question_classifier: on_topic = {state['on_topic']}\")\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    print(\"Entering on_topic_router\")\n",
    "    confidence_score = state.get(\"on_topic\", 0.0)\n",
    "\n",
    "    if confidence_score >= 0.5:\n",
    "        print(f\"Routing to retrieve with Confidence Score: {confidence_score}\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(f\"Routing to off_topic_response with Confidence Score: {confidence_score}\")\n",
    "        return \"off_topic_response\"\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"Entering retrieve\")\n",
    "    documents = retriever.invoke(state[\"rephrased_question\"])\n",
    "    print(f\"retrieve: Retrieved {len(documents)} documents\")\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Document is relevant to the question? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    print(\"Entering retrieval_grader\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "Only answer with 'Yes' or 'No'.\n",
    "\n",
    "If the document contains information relevant to the user's question, respond with 'Yes'.\n",
    "Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        human_message = HumanMessage(\n",
    "            content=f\"User question: {state['rephrased_question']}\\n\\nRetrieved document:\\n{doc.page_content}\"\n",
    "        )\n",
    "        grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "        grader_llm = grade_prompt | structured_llm\n",
    "        result = grader_llm.invoke({})\n",
    "        print(\n",
    "            f\"Grading document: {doc.page_content[:30]}... Result: {result.score.strip()}\"\n",
    "        )\n",
    "        if result.score.strip().lower() == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    state[\"documents\"] = relevant_docs\n",
    "    state[\"proceed_to_generate\"] = len(relevant_docs) > 0\n",
    "    print(f\"retrieval_grader: proceed_to_generate = {state['proceed_to_generate']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    print(\"Entering proceed_router\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if state.get(\"proceed_to_generate\", False):\n",
    "        print(\"Routing to generate_answer\")\n",
    "        return \"generate_answer\"\n",
    "    elif rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached. Cannot find relevant documents.\")\n",
    "        return \"cannot_answer\"\n",
    "    else:\n",
    "        print(\"Routing to refine_question\")\n",
    "        return \"refine_question\"\n",
    "\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    print(\"Entering refine_question\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached\")\n",
    "        return state\n",
    "    question_to_refine = state[\"rephrased_question\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that slightly refines the user's question to improve retrieval results.\n",
    "Provide a slightly adjusted version of the question.\"\"\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Original question: {question_to_refine}\\n\\nProvide a slightly refined question.\"\n",
    "    )\n",
    "    refine_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    prompt = refine_prompt.format()\n",
    "    response = llm.invoke(prompt)\n",
    "    refined_question = response.content.strip()\n",
    "    print(f\"refine_question: Refined question: {refined_question}\")\n",
    "    state[\"rephrased_question\"] = refined_question\n",
    "    state[\"rephrase_count\"] = rephrase_count + 1\n",
    "    return state\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    print(\"Entering generate_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        raise ValueError(\"State must include 'messages' before generating an answer.\")\n",
    "\n",
    "    history = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"history\": history, \"context\": documents, \"question\": rephrased_question}\n",
    "    )\n",
    "\n",
    "    response_text = response.content.strip()\n",
    "\n",
    "    # Fun, engaging follow-up lines\n",
    "    fun_replies = [\n",
    "        \"Hope that helps! Need more travel tips? 😊\",\n",
    "        \"That’s the scoop! Got any other questions? ✈️\",\n",
    "        \"Wanna know more? Just ask! 🌍\",\n",
    "        \"If you’re curious about anything else, I’m all ears! 👂\",\n",
    "        \"Travel talk is my jam! Hit me up with another question. 🚀\"\n",
    "    ]\n",
    "\n",
    "    # Add a random fun reply to the response\n",
    "    generation = f\"{response_text} {random.choice(fun_replies)}\"\n",
    "\n",
    "    state[\"messages\"].append(AIMessage(content=generation))\n",
    "    print(f\"generate_answer: Generated response: {generation}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(\n",
    "            content=\"I'm sorry, but I cannot find the information you're looking for.\"\n",
    "        )\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(AIMessage(content=\"Oof, I wish I could help with that! But I’m all about travel. Wanna ask me about flights, hotels, or dreamy destinations instead? 😎✈️\"))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cf5ec1-8a0e-410d-a99f-50852f4c0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bab885-a152-459b-bee3-5f49be1afad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"question_rewriter\", question_rewriter)\n",
    "workflow.add_node(\"question_classifier\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"retrieval_grader\", retrieval_grader)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"refine_question\", refine_question)\n",
    "workflow.add_node(\"cannot_answer\", cannot_answer)\n",
    "\n",
    "workflow.add_edge(\"question_rewriter\", \"question_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_classifier\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"off_topic_response\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"retrieval_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieval_grader\",\n",
    "    proceed_router,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"refine_question\": \"refine_question\",\n",
    "        \"cannot_answer\": \"cannot_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"refine_question\", \"retrieve\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"cannot_answer\", END)\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.set_entry_point(\"question_rewriter\")\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a76d10-d30f-4864-930b-557232c44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fdcbb-c1b4-46a4-a204-8a2fcdf6f422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c085f7-1aac-491d-86dd-36a5d66aeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = {\n",
    "#     \"question\": HumanMessage(content=\"what are the popular destinations to visit?\")\n",
    "# }\n",
    "# graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483aef18-129b-4829-8e4b-a4e48e6bd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = {\"question\": HumanMessage(content=\"in that suggest me one?\")}\n",
    "# graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e303f3-d70a-4395-814a-3093c9e2525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3771cd-3569-41d0-bd99-0a2926fcf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "658e0572-4b59-47d7-b019-90d92cdbe14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "user_name = None\n",
    "\n",
    "def extract_name(question):\n",
    "\n",
    "    patterns = [\n",
    "        r\"\\bmy name is (\\w+)\\b\",\n",
    "        r\"\\bi am (\\w+)\\b\",\n",
    "        r\"\\bthis is (\\w+)\\b\",\n",
    "        r\"\\bcall me (\\w+)\\b\",\n",
    "        r\"\\bi'm (\\w+)\\b\",\n",
    "        r\"\\bthey call me (\\w+)\\b\",\n",
    "        r\"\\bthe name's (\\w+)\\b\",\n",
    "        r\"\\bname me (\\w+)\\b\",\n",
    "        r\"\\bjust call me (\\w+)\\b\",\n",
    "        r\"\\bpeople call me (\\w+)\\b\",\n",
    "        r\"\\byou can call me (\\w+)\\b\",\n",
    "        r\"\\bhello, (\\w+)\\b\",\n",
    "        r\"\\bhey, (\\w+)\\b\",\n",
    "        r\"\\bhi, i'm (\\w+)\\b\",\n",
    "        r\"\\bit's (\\w+)\\b\",\n",
    "        r\"\\bmy friends call me (\\w+)\\b\",\n",
    "        r\"\\bknown as (\\w+)\\b\",\n",
    "        r\"^(\\w+)$\"\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, question, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_question(question: str, thread_id: int = 1):\n",
    "    global user_name\n",
    "    \n",
    "    # Check for greetings\n",
    "    if any(greet in question.lower() for greet in [\"hi\", \"hello\", \"hey\"]):\n",
    "        if user_name:\n",
    "            return f\"Hey {user_name}! Welcome back! What’s on your travel wishlist today? 🌍✈️\"\n",
    "        return \"Hey there! What should I call you? 😊\"\n",
    "    \n",
    "    # Check if user introduced themselves\n",
    "    name = extract_name(question)\n",
    "    if name:\n",
    "        user_name = name\n",
    "        return f\"Nice to meet you, {user_name}! Ask me anything about travel. 🌍✈️\"\n",
    "\n",
    "    input_data = {\"question\": HumanMessage(content=question)}\n",
    "    result = graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "    \n",
    "    messages = result.get(\"messages\", [])\n",
    "    ai_responses = [msg.content for msg in messages if isinstance(msg, AIMessage)]\n",
    "    \n",
    "    return ai_responses[-1] if ai_responses else \"Sorry, I couldn't find an answer.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9644a5a-b414-4bb1-996b-dc913b7c1fd4",
   "metadata": {},
   "source": [
    "**version 0 for gradio UI - works well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3563ea34-b3e0-4fcc-a354-e49efc62bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import gradio as gr\n",
    "\n",
    "# def gradio_interface(question):\n",
    "#     \"\"\"\n",
    "#     Gradio chat interface function to process user questions.\n",
    "#     \"\"\"\n",
    "#     return process_question(question)\n",
    "\n",
    "# # Create Gradio Chat UI\n",
    "# with gr.Blocks(theme=gr.themes.Soft()) as ui:\n",
    "#     gr.Markdown(\"# ✈️ Travel Assistant\")\n",
    "#     gr.Markdown(\"Ask me anything about flights, hotels, visas, travel packages, and more!\")\n",
    "\n",
    "#     chatbot = gr.Chatbot(label=\"Travel Assistant\", type=\"messages\")\n",
    "#     msg = gr.Textbox(placeholder=\"Type your question here...\", label=\"Your Question\")\n",
    "\n",
    "#     def chat_function(messages, message):\n",
    "#         response = gradio_interface(message)\n",
    "#         messages.append({\"role\": \"user\", \"content\": message})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "#         return messages, \"\"\n",
    "\n",
    "#     msg.submit(chat_function, [chatbot, msg], [chatbot, msg])\n",
    "\n",
    "# # Launch UI\n",
    "# ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f727a06-0eee-4d6b-b25d-9ba39aede049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546cc55-708c-4011-b90b-12ebc4edaa17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71588a5b-7c37-425f-84e6-47b7545535f3",
   "metadata": {},
   "source": [
    "**Testing Image Generation - works well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f61509-d080-453a-92c3-c52db36ac8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from io import BytesIO\n",
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "089297f4-7052-435c-9a85-1c6198a51748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def artist(city):\n",
    "#     image_response = openai.images.generate(\n",
    "#             model=\"dall-e-3\",\n",
    "#             prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "#             size=\"1024x1024\",\n",
    "#             n=1,\n",
    "#             response_format=\"b64_json\",\n",
    "#         )\n",
    "#     image_base64 = image_response.data[0].b64_json\n",
    "#     image_data = base64.b64decode(image_base64)\n",
    "#     return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acadaa83-ab9c-420c-80a3-79a1b1181625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = artist(\"india\")\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df1ed3-2962-423b-8c0f-25b65bcf8d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50897cda-c5db-4b2e-8b64-dc480f931b74",
   "metadata": {},
   "source": [
    "**Testing text to speech for the responses - works well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8bb19b-b231-458e-9062-f1503c9dbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# from IPython.display import Audio, display\n",
    "\n",
    "# def talker(message):\n",
    "#     response = openai.audio.speech.create(\n",
    "#         model=\"tts-1\",\n",
    "#         voice=\"onyx\",\n",
    "#         input=message)\n",
    "\n",
    "#     audio_stream = BytesIO(response.content)\n",
    "#     output_filename = \"output_audio.mp3\"\n",
    "#     with open(output_filename, \"wb\") as f:\n",
    "#         f.write(audio_stream.read())\n",
    "\n",
    "#     # Play the generated audio\n",
    "#     display(Audio(output_filename, autoplay=True))\n",
    "\n",
    "# talker(\"Well, hi madhan how are you. its been so long we met each other. nice to meet you..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33227753-a615-4784-8ce4-fdc608756c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba0dff-2723-4b6d-b3a6-eb8f39244818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e34ba29-6c12-4d45-a1df-fcaba0f83d58",
   "metadata": {},
   "source": [
    "**version 1 Merging the image and audio generation with gradio ui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d0664d-b1dc-429d-b9bf-d425179de95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# import base64\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# from IPython.display import Audio, display\n",
    "# from openai import OpenAI\n",
    "# import requests\n",
    "\n",
    "# # OpenAI API client\n",
    "# client = OpenAI()\n",
    "\n",
    "# # Track whether an image should be generated\n",
    "# image_context = None\n",
    "# generated_image = None\n",
    "\n",
    "\n",
    "# def generate_image(prompt):\n",
    "#     global image_context, generated_image\n",
    "    \n",
    "#     # If the context hasn't changed, return the existing image\n",
    "#     if prompt == image_context and generated_image is not None:\n",
    "#         return generated_image\n",
    "    \n",
    "#     # Otherwise, generate a new image\n",
    "#     image_response = client.images.generate(\n",
    "#         model=\"dall-e-3\",\n",
    "#         # prompt=f\"A travel-themed image representing {prompt}, featuring landmarks, nature, and vibrant scenery.\",\n",
    "#         prompt=f\"A modern comic book panel illustration of a travel-themed scene representing {prompt}, featuring landmarks, cityscapes, or nature with vibrant colors, bold outlines, and dynamic storytelling. Characters interacting with the environment, detailed textures, and expressive poses. Highly detailed with shading and comic-like dialogue elements.\",\n",
    "#         size=\"1024x1024\",\n",
    "#         n=1,\n",
    "#         response_format=\"b64_json\",\n",
    "#     )\n",
    "    \n",
    "#     image_base64 = image_response.data[0].b64_json\n",
    "#     image_data = base64.b64decode(image_base64)\n",
    "    \n",
    "#     # Update the context and store the image\n",
    "#     image_context = prompt\n",
    "#     generated_image = Image.open(BytesIO(image_data))\n",
    "#     return generated_image\n",
    "\n",
    "\n",
    "# def generate_audio(text):\n",
    "#     response = client.audio.speech.create(\n",
    "#         model=\"tts-1\",\n",
    "#         voice=\"onyx\",\n",
    "#         input=text\n",
    "#     )\n",
    "    \n",
    "#     audio_stream = BytesIO(response.content)\n",
    "#     output_filename = \"output_audio.mp3\"\n",
    "#     with open(output_filename, \"wb\") as f:\n",
    "#         f.write(audio_stream.read())\n",
    "\n",
    "#     return output_filename\n",
    "\n",
    "\n",
    "# def chat_with_options(messages, message, image_toggle, audio_toggle):\n",
    "#     \"\"\"\n",
    "#     Handles chat input and generates responses.\n",
    "#     Also triggers image and audio generation if enabled.\n",
    "#     \"\"\"\n",
    "#     global generated_image\n",
    "    \n",
    "#     response_text = process_question(message)  # Call the main chatbot function\n",
    "    \n",
    "#     image_output = None\n",
    "#     audio_output = None\n",
    "    \n",
    "#     if image_toggle:\n",
    "#         image_output = generate_image(message)\n",
    "\n",
    "#     if audio_toggle:\n",
    "#         audio_output = generate_audio(response_text)\n",
    "    \n",
    "#     # Append chat messages\n",
    "#     messages.append({\"role\": \"user\", \"content\": message})\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "#     return messages, \"\", image_output, audio_output\n",
    "\n",
    "\n",
    "# # Gradio UI with two sections\n",
    "# with gr.Blocks(theme=gr.themes.Soft()) as ui:\n",
    "#     gr.Markdown(\"# ✈️ Travel Assistant\")\n",
    "#     gr.Markdown(\"Ask me anything about flights, hotels, visas, travel packages, and more!\")\n",
    "\n",
    "#     with gr.Row():  # Split UI\n",
    "#         with gr.Column(scale=2):  # Chat section\n",
    "#             chatbot = gr.Chatbot(label=\"Travel Assistant\", type=\"messages\")\n",
    "#             msg = gr.Textbox(placeholder=\"Type your question here...\", label=\"Your Question\")\n",
    "#             with gr.Row():\n",
    "#                 img_toggle = gr.Checkbox(label=\"Generate Image\")\n",
    "#                 audio_toggle = gr.Checkbox(label=\"Enable Audio Response\")\n",
    "\n",
    "#         with gr.Column(scale=1):  # Image display\n",
    "#             image_display = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "#     # Audio output\n",
    "#     audio_output = gr.Audio(label=\"Response Audio\", autoplay=True)\n",
    "\n",
    "#     # Chat function with toggles\n",
    "#     msg.submit(chat_with_options, [chatbot, msg, img_toggle, audio_toggle], [chatbot, msg, image_display, audio_output])\n",
    "\n",
    "# # Launch UI\n",
    "# ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1d033-036b-4e2b-86a5-2bb08ff0972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027aefd1-0d6c-4c6d-8218-a4c283b3450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf31da2-abf3-4015-ae23-30b65b4af596",
   "metadata": {},
   "source": [
    "**version 2 for merging audio and video with Gradio UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62250574-c37e-42c1-9199-7a9ea4a620d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# import base64\n",
    "# import os\n",
    "# import time\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# from openai import OpenAI\n",
    "# import pygame\n",
    "\n",
    "# # Initialize OpenAI API client\n",
    "# client = OpenAI()\n",
    "\n",
    "# # Create directories for saving images and audio files\n",
    "# os.makedirs(\"image_files\", exist_ok=True)\n",
    "# os.makedirs(\"audio_files\", exist_ok=True)\n",
    "\n",
    "# # Track generated image to prevent unnecessary re-generation\n",
    "# image_context = None\n",
    "# generated_image = None\n",
    "\n",
    "# def get_timestamp():\n",
    "#     \"\"\"Generate a timestamp for unique file naming.\"\"\"\n",
    "#     return time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "# def generate_image(prompt):\n",
    "#     \"\"\"Generate and save an image based on the user's prompt.\"\"\"\n",
    "#     global image_context, generated_image\n",
    "\n",
    "#     if prompt == image_context and generated_image is not None:\n",
    "#         return generated_image\n",
    "\n",
    "#     # Generate new image from OpenAI\n",
    "#     image_response = client.images.generate(\n",
    "#         model=\"dall-e-3\",\n",
    "#         prompt=f\"A modern comic book panel illustration of a travel-themed scene representing {prompt}, featuring landmarks, cityscapes, or nature with vibrant colors, bold outlines, and dynamic storytelling. Characters interacting with the environment, detailed textures, and expressive poses. Highly detailed with shading and comic-like dialogue elements.\",\n",
    "#         size=\"1024x1024\",\n",
    "#         n=1,\n",
    "#         response_format=\"b64_json\",\n",
    "#     )\n",
    "\n",
    "#     image_base64 = image_response.data[0].b64_json\n",
    "#     image_data = base64.b64decode(image_base64)\n",
    "    \n",
    "#     # Save image with unique filename\n",
    "#     timestamp = get_timestamp()\n",
    "#     image_path = f\"image_files/travel_image_{timestamp}.png\"\n",
    "#     with open(image_path, \"wb\") as f:\n",
    "#         f.write(image_data)\n",
    "    \n",
    "#     image_context = prompt\n",
    "#     generated_image = Image.open(image_path)\n",
    "#     return generated_image\n",
    "\n",
    "# def generate_audio(text):\n",
    "#     \"\"\"Generate and save audio, then play it in the background.\"\"\"\n",
    "#     response = client.audio.speech.create(\n",
    "#         model=\"tts-1\",\n",
    "#         voice=\"alloy\",\n",
    "#         input=text\n",
    "#     )\n",
    "    \n",
    "#     timestamp = get_timestamp()\n",
    "#     audio_path = f\"audio_files/response_audio_{timestamp}.mp3\"\n",
    "    \n",
    "#     with open(audio_path, \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "    \n",
    "#     # Play audio in background\n",
    "#     pygame.mixer.init()\n",
    "#     pygame.mixer.music.load(audio_path)\n",
    "#     pygame.mixer.music.play()\n",
    "    \n",
    "#     return audio_path  # Returning for reference if needed, but not displaying\n",
    "\n",
    "# def chat_with_options(messages, message, image_toggle, audio_toggle):\n",
    "#     \"\"\"Handles chat and triggers image and audio generation if enabled.\"\"\"\n",
    "#     global generated_image\n",
    "\n",
    "#     response_text = process_question(message)  # Replace with your chatbot logic\n",
    "    \n",
    "#     image_output = None\n",
    "\n",
    "#     if image_toggle:\n",
    "#         image_output = generate_image(message)\n",
    "\n",
    "#     if audio_toggle:\n",
    "#         generate_audio(response_text)  # Play audio in the background\n",
    "\n",
    "#     messages.append({\"role\": \"user\", \"content\": message})\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "#     return messages, \"\", image_output  # Remove audio_output as it plays in the background\n",
    "\n",
    "# # Gradio UI\n",
    "# with gr.Blocks(theme=gr.themes.Soft()) as ui:\n",
    "#     gr.Markdown(\"# ✈️ Travel Assistant\")\n",
    "#     gr.Markdown(\"Ask me anything about flights, hotels, visas, travel packages, and more!\")\n",
    "\n",
    "#     with gr.Row():  \n",
    "#         with gr.Column(scale=2):  \n",
    "#             chatbot = gr.Chatbot(label=\"Travel Assistant\", type=\"messages\")\n",
    "#             msg = gr.Textbox(placeholder=\"Type your question here...\", label=\"Your Question\")\n",
    "#             with gr.Row():\n",
    "#                 img_toggle = gr.Checkbox(label=\"Generate Image\")\n",
    "#                 audio_toggle = gr.Checkbox(label=\"Enable Audio Response\")\n",
    "\n",
    "#         with gr.Column(scale=1):  \n",
    "#             image_display = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "#     msg.submit(chat_with_options, [chatbot, msg, img_toggle, audio_toggle], [chatbot, msg, image_display])\n",
    "\n",
    "# ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a087445d-4af8-487d-b20b-0accc98fcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\91637\\anaconda3\\envs\\llms\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621766c5-b2e9-4da2-85f1-f9e7ac9a3933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbaac26-6ea5-4f43-9d92-c7d1d770d6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb88be2-2f32-4a11-955a-30fa5331326c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703bac0c-4eb0-4e07-bb7f-9d63090c450e",
   "metadata": {},
   "source": [
    "**version 3 for merging audio and image with gradio ui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4ae4d-9918-436a-8d1d-f1926b7ba750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import base64\n",
    "import os\n",
    "import time\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import pygame\n",
    "\n",
    "# Initialize OpenAI API client\n",
    "client = OpenAI()\n",
    "\n",
    "# Create directories for saving images and audio files\n",
    "os.makedirs(\"image_files\", exist_ok=True)\n",
    "os.makedirs(\"audio_files\", exist_ok=True)\n",
    "\n",
    "# Track generated image and context\n",
    "image_context = None\n",
    "generated_image = None\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Generate a timestamp for unique file naming.\"\"\"\n",
    "    return time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "def is_same_context(prev_prompt, new_prompt):\n",
    "    \"\"\"Determine if two prompts belong to the same context using OpenAI embeddings.\"\"\"\n",
    "    if not prev_prompt:\n",
    "        return False  # No previous context, treat as new\n",
    "\n",
    "    embedding_response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=[prev_prompt, new_prompt]\n",
    "    )\n",
    "    embeddings = embedding_response.data\n",
    "    similarity_score = sum(a * b for a, b in zip(embeddings[0].embedding, embeddings[1].embedding))\n",
    "    \n",
    "    return similarity_score > 0.9  # Adjust threshold as needed\n",
    "\n",
    "def generate_image(prompt):\n",
    "    \"\"\"Generate and save an image based on the user's prompt.\"\"\"\n",
    "    global image_context, generated_image\n",
    "\n",
    "    if image_context and is_same_context(image_context, prompt):\n",
    "        return generated_image  # Keep the previous image if context is the same\n",
    "\n",
    "    # Generate new image from OpenAI\n",
    "    image_response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=f\"A modern comic book panel illustration of a travel-themed scene representing {prompt}, featuring landmarks, cityscapes, or nature with vibrant colors, bold outlines, and dynamic storytelling.\",\n",
    "        size=\"1024x1024\",\n",
    "        n=1,\n",
    "        response_format=\"b64_json\",\n",
    "    )\n",
    "\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    \n",
    "    # Save image with unique filename\n",
    "    timestamp = get_timestamp()\n",
    "    image_path = f\"image_files/travel_image_{timestamp}.png\"\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(image_data)\n",
    "    \n",
    "    image_context = prompt\n",
    "    generated_image = Image.open(image_path)\n",
    "    return generated_image\n",
    "\n",
    "def generate_audio(text):\n",
    "    \"\"\"Generate and save audio, then play it in the background.\"\"\"\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text\n",
    "    )\n",
    "    \n",
    "    timestamp = get_timestamp()\n",
    "    audio_path = f\"audio_files/response_audio_{timestamp}.mp3\"\n",
    "    \n",
    "    with open(audio_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Play audio in background\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(audio_path)\n",
    "    pygame.mixer.music.play()\n",
    "    \n",
    "    return audio_path  # Returning for reference if needed, but not displaying\n",
    "\n",
    "def chat_with_options(messages, message, image_toggle, audio_toggle):\n",
    "    \"\"\"Handles chat and triggers image and audio generation if enabled.\"\"\"\n",
    "    global generated_image\n",
    "\n",
    "    response_text = process_question(message)  # Replace with your chatbot logic\n",
    "    \n",
    "    image_output = None\n",
    "    if generated_image and is_same_context(image_context, message):\n",
    "        image_output = generated_image  # Keep previous image\n",
    "    elif image_toggle:\n",
    "        image_output = generate_image(message)  # Generate new image only if enabled\n",
    "\n",
    "    if audio_toggle:\n",
    "        generate_audio(response_text)  # Play audio in the background\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "    return messages, \"\", image_output  # Remove audio_output as it plays in the background\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as ui:\n",
    "    gr.Markdown(\"# ✈️ Travel Assistant\")\n",
    "    gr.Markdown(\"Ask me anything about flights, hotels, visas, travel packages, and more!\")\n",
    "\n",
    "    with gr.Row():  \n",
    "        with gr.Column(scale=2):  \n",
    "            chatbot = gr.Chatbot(label=\"Travel Assistant\", type=\"messages\")\n",
    "            msg = gr.Textbox(placeholder=\"Type your question here...\", label=\"Your Question\")\n",
    "            with gr.Row():\n",
    "                img_toggle = gr.Checkbox(label=\"Generate Image\")\n",
    "                audio_toggle = gr.Checkbox(label=\"Enable Audio Response\")\n",
    "\n",
    "        with gr.Column(scale=1):  \n",
    "            image_display = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "    msg.submit(chat_with_options, [chatbot, msg, img_toggle, audio_toggle], [chatbot, msg, image_display])\n",
    "\n",
    "ui.launch(share=True, debug = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063ed4b-e497-4201-a3df-c869c014e192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52a58a-396a-44bb-8c4a-417a6d4cbe50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c2dea-0b81-41e1-9db8-673cd1da48be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abe836-93bf-4551-9f0c-9814693abf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c6a56-0b9e-4e61-9fd2-93075896e184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2bd9e-04aa-4f3c-b6d7-487d547d0789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
